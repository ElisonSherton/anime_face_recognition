{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we shall be using Siamese Network in order to build a model to perform the task of face verification for a given character.\n",
    "\n",
    "The paper referred to for performing this experiment is [linked here](https://proceedings.neurips.cc/paper/1993/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siameseDataset import *\n",
    "from loss_func import *\n",
    "from siameseModel import *\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "import pandas as pd\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/vinayak/cleaned_anime_faces\"\n",
    "model_save_path = \"/home/vinayak/anime_face_recognition/enet_model.pth\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = {}\n",
    "split_info = pd.read_csv(f\"/home/vinayak/anime_face_recognition/data.csv\")\n",
    "partition[\"train\"] = list(split_info[split_info.label == \"train\"].images)\n",
    "random.shuffle(partition[\"train\"])\n",
    "partition[\"validation\"] = list(split_info[split_info.label == \"valid\"].images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://omoindrot.github.io/triplet-loss#strategies-in-online-mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training dataset and use it to create a training_generator\n",
    "training_set = siameseDataset(partition['train'])\n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size = 1)\n",
    "# training_set.show_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training dataset and use it to create a validation_generator\n",
    "validation_set = siameseDataset(partition['validation'], dtype = \"validation\")\n",
    "validation_generator = torch.utils.data.DataLoader(validation_set, batch_size = 1)\n",
    "# validation_set.show_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and move it to appropriate device (i.e. cuda if gpu is available)\n",
    "model = enet_model().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function to be used for training\n",
    "loss_func = batchHardTripletLoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a learning rate and create an optimizer for training the model \n",
    "# (Adam with default momentum should be good)\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Define a learning rate scheduler so that you reduce the learning rate\n",
    "# As you progress across multiple epochs\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 3, factor = 0.2, threshold = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1   | Batch Number: 10  | Current Batch Loss: 2.7512 | Average Train Loss: 2.32116\n",
      "Epoch: 1   | Batch Number: 20  | Current Batch Loss: 1.92464| Average Train Loss: 2.1912 \n",
      "Epoch: 1   | Batch Number: 30  | Current Batch Loss: 1.64174| Average Train Loss: 2.12889\n",
      "Epoch: 1   | Batch Number: 40  | Current Batch Loss: 1.53397| Average Train Loss: 2.05252\n",
      "Epoch: 1   | Batch Number: 50  | Current Batch Loss: 1.22877| Average Train Loss: 1.96055\n",
      "Epoch: 1   | Batch Number: 54  | Current Batch Loss: 1.37387| Average Train Loss: 1.92623\n",
      "####################### End of Epoch 1 #######################\n",
      "Epoch: 1   | Train Loss: 1.91822| Valid Loss: 1.42327\n",
      "\n",
      "Epoch: 2   | Batch Number: 10  | Current Batch Loss: 1.40383| Average Train Loss: 1.44146\n",
      "Epoch: 2   | Batch Number: 20  | Current Batch Loss: 1.34176| Average Train Loss: 1.35352\n",
      "Epoch: 2   | Batch Number: 30  | Current Batch Loss: 1.14791| Average Train Loss: 1.31153\n",
      "Epoch: 2   | Batch Number: 40  | Current Batch Loss: 1.12557| Average Train Loss: 1.27279\n",
      "Epoch: 2   | Batch Number: 50  | Current Batch Loss: 1.19358| Average Train Loss: 1.25282\n",
      "Epoch: 2   | Batch Number: 54  | Current Batch Loss: 1.13418| Average Train Loss: 1.24595\n",
      "####################### End of Epoch 2 #######################\n",
      "Epoch: 2   | Train Loss: 1.24579| Valid Loss: 1.18339\n",
      "\n",
      "Epoch: 3   | Batch Number: 10  | Current Batch Loss: 1.08489| Average Train Loss: 1.11179\n",
      "Epoch: 3   | Batch Number: 20  | Current Batch Loss: 1.06341| Average Train Loss: 1.09675\n",
      "Epoch: 3   | Batch Number: 30  | Current Batch Loss: 1.062  | Average Train Loss: 1.08582\n",
      "Epoch: 3   | Batch Number: 40  | Current Batch Loss: 1.04317| Average Train Loss: 1.07913\n",
      "Epoch: 3   | Batch Number: 50  | Current Batch Loss: 1.02129| Average Train Loss: 1.07115\n",
      "Epoch: 3   | Batch Number: 54  | Current Batch Loss: 1.03695| Average Train Loss: 1.06872\n",
      "####################### End of Epoch 3 #######################\n",
      "Epoch: 3   | Train Loss: 1.06796| Valid Loss: 1.0316 \n",
      "\n",
      "Epoch: 4   | Batch Number: 10  | Current Batch Loss: 1.00204| Average Train Loss: 1.029  \n",
      "Epoch: 4   | Batch Number: 20  | Current Batch Loss: 1.016  | Average Train Loss: 1.02623\n",
      "Epoch: 4   | Batch Number: 30  | Current Batch Loss: 1.01413| Average Train Loss: 1.0247 \n",
      "Epoch: 4   | Batch Number: 40  | Current Batch Loss: 1.0185 | Average Train Loss: 1.02346\n",
      "Epoch: 4   | Batch Number: 50  | Current Batch Loss: 1.01659| Average Train Loss: 1.02206\n",
      "Epoch: 4   | Batch Number: 54  | Current Batch Loss: 1.0206 | Average Train Loss: 1.02155\n",
      "####################### End of Epoch 4 #######################\n",
      "Epoch: 4   | Train Loss: 1.02141| Valid Loss: 1.0191 \n",
      "\n",
      "Epoch: 5   | Batch Number: 10  | Current Batch Loss: 1.0138 | Average Train Loss: 1.01832\n",
      "Epoch: 5   | Batch Number: 20  | Current Batch Loss: 1.01665| Average Train Loss: 1.01875\n",
      "Epoch: 5   | Batch Number: 30  | Current Batch Loss: 1.00928| Average Train Loss: 1.01596\n",
      "Epoch: 5   | Batch Number: 40  | Current Batch Loss: 1.012  | Average Train Loss: 1.014  \n",
      "Epoch: 5   | Batch Number: 50  | Current Batch Loss: 1.00596| Average Train Loss: 1.01313\n",
      "Epoch: 5   | Batch Number: 54  | Current Batch Loss: 1.00944| Average Train Loss: 1.01297\n",
      "####################### End of Epoch 5 #######################\n",
      "Epoch: 5   | Train Loss: 1.01298| Valid Loss: 1.01144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "n_epochs = 5\n",
    "n_train_batches = len(training_generator)\n",
    "n_valid_batches = len(validation_generator)\n",
    "PRINT_PROGRESS = 10\n",
    "\n",
    "round_off = lambda x: round(x, 5)\n",
    "\n",
    "# Loop over number of epochs\n",
    "for epch in range(n_epochs):\n",
    "    \n",
    "    # Initialize the loss values to zero at the beginning of the epoch\n",
    "    train_loss = 0.\n",
    "    valid_loss = 0.\n",
    "\n",
    "    # Train for an epoch\n",
    "    for idx, (images, labels) in enumerate(training_generator, start = 1):\n",
    "        images, labels = images[0].to(DEVICE), labels.to(DEVICE)\n",
    "        feature_vectors = model(images)\n",
    "        loss = loss_func(feature_vectors, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = round_off(loss.item())\n",
    "        train_loss += batch_loss\n",
    "        \n",
    "        if (idx % PRINT_PROGRESS == 0) or (idx == 0) or (idx == n_train_batches - 1):\n",
    "            print(f\"Epoch: {(epch + 1):<4}| Batch Number: {idx:<4}| Current Batch Loss: {batch_loss:<7}| Average Train Loss: {round_off(train_loss / idx):<7}\")\n",
    "    \n",
    "    # Validate after the trained epoch\n",
    "    for images, labels in validation_generator:\n",
    "        images, labels = images[0].to(DEVICE), labels.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            feature_vectors = model(images)\n",
    "            loss = loss_func(feature_vectors, labels)\n",
    "            valid_loss += round_off(loss.item())\n",
    "    \n",
    "    # Average the train and valid losses across all batches and save it to our array\n",
    "    train_loss = round_off(train_loss / n_train_batches)\n",
    "    valid_loss = round_off(valid_loss / n_valid_batches)\n",
    "    \n",
    "    print(f\"####################### End of Epoch {epch + 1} #######################\")\n",
    "    print(f\"Epoch: {(epch + 1):<4}| Train Loss: {train_loss:<7}| Valid Loss: {valid_loss:<7}\")\n",
    "    print()\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    # Check the valid loss and reduce learning rate as per the need\n",
    "    scheduler.step(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the losses to a loss_history.csv file on the disk\n",
    "history = pd.DataFrame({\"train_loss\": train_losses, \"valid_loss\":valid_losses})\n",
    "history.to_csv(\"loss_history.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to our disk\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
