{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we shall be using Siamese Network in order to build a model to perform the task of face verification for a given character.\n",
    "\n",
    "The paper referred to for performing this experiment is [linked here](https://proceedings.neurips.cc/paper/1993/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siameseDataset import *\n",
    "from loss_func import *\n",
    "from siameseModel import *\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "import pandas as pd\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/vinayak/cleaned_anime_faces\"\n",
    "model_save_path = \"/home/vinayak/anime_face_recognition/enet_model.pth\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = {}\n",
    "split_info = pd.read_csv(f\"/home/vinayak/anime_face_recognition/data.csv\")\n",
    "partition[\"train\"] = list(split_info[split_info.label == \"train\"].images)\n",
    "random.shuffle(partition[\"train\"])\n",
    "partition[\"validation\"] = list(split_info[split_info.label == \"valid\"].images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://omoindrot.github.io/triplet-loss#strategies-in-online-mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training dataset and use it to create a training_generator\n",
    "training_set = siameseDataset(partition['train'])\n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size = 1)\n",
    "# training_set.show_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training dataset and use it to create a validation_generator\n",
    "validation_set = siameseDataset(partition['validation'], dtype = \"validation\")\n",
    "validation_generator = torch.utils.data.DataLoader(validation_set, batch_size = 1)\n",
    "# validation_set.show_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and move it to appropriate device (i.e. cuda if gpu is available)\n",
    "model = enet_model().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function to be used for training\n",
    "loss_func = batchHardTripletLoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a learning rate and create an optimizer for training the model \n",
    "# (Adam with default momentum should be good)\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Define a learning rate scheduler so that you reduce the learning rate\n",
    "# As you progress across multiple epochs\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 3, factor = 0.2, threshold = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0   | Batch Number: 10  | Current Batch Loss: 1.89903| Average Train Loss: 2.26895\n",
      "Epoch: 0   | Batch Number: 20  | Current Batch Loss: 1.69898| Average Train Loss: 2.0447 \n",
      "Epoch: 0   | Batch Number: 30  | Current Batch Loss: 2.09667| Average Train Loss: 2.0243 \n",
      "Epoch: 0   | Batch Number: 40  | Current Batch Loss: 1.3731 | Average Train Loss: 1.92716\n",
      "Epoch: 0   | Batch Number: 50  | Current Batch Loss: 1.3483 | Average Train Loss: 1.8123 \n",
      "####################### End of Epoch 1 #######################\n",
      "Epoch: 0   | Train Loss: 1.76104| Valid Loss: 1.26766\n",
      "#######################################################################\n",
      "Epoch: 1   | Batch Number: 10  | Current Batch Loss: 1.16743| Average Train Loss: 1.2281 \n",
      "Epoch: 1   | Batch Number: 20  | Current Batch Loss: 1.10261| Average Train Loss: 1.17803\n",
      "Epoch: 1   | Batch Number: 30  | Current Batch Loss: 1.05385| Average Train Loss: 1.14313\n",
      "Epoch: 1   | Batch Number: 40  | Current Batch Loss: 1.08707| Average Train Loss: 1.12632\n",
      "Epoch: 1   | Batch Number: 50  | Current Batch Loss: 1.08678| Average Train Loss: 1.11668\n",
      "####################### End of Epoch 2 #######################\n",
      "Epoch: 1   | Train Loss: 1.11185| Valid Loss: 1.0585 \n",
      "#######################################################################\n",
      "Epoch: 2   | Batch Number: 10  | Current Batch Loss: 1.05298| Average Train Loss: 1.06214\n",
      "Epoch: 2   | Batch Number: 20  | Current Batch Loss: 1.05175| Average Train Loss: 1.06536\n",
      "Epoch: 2   | Batch Number: 30  | Current Batch Loss: 1.04895| Average Train Loss: 1.05978\n",
      "Epoch: 2   | Batch Number: 40  | Current Batch Loss: 1.02524| Average Train Loss: 1.05425\n",
      "Epoch: 2   | Batch Number: 50  | Current Batch Loss: 1.01836| Average Train Loss: 1.04968\n",
      "####################### End of Epoch 3 #######################\n",
      "Epoch: 2   | Train Loss: 1.04718| Valid Loss: 1.02437\n",
      "#######################################################################\n",
      "Epoch: 3   | Batch Number: 10  | Current Batch Loss: 1.02343| Average Train Loss: 1.02265\n",
      "Epoch: 3   | Batch Number: 20  | Current Batch Loss: 1.02169| Average Train Loss: 1.02066\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'SqrtBackward' returned nan values in its 0th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-50a1291e308b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mfeature_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anime/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anime/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'SqrtBackward' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "n_epochs = 5\n",
    "n_train_batches = len(training_generator)\n",
    "n_valid_batches = len(validation_generator)\n",
    "PRINT_PROGRESS = 10\n",
    "\n",
    "round_off = lambda x: round(x, 5)\n",
    "\n",
    "# Loop over number of epochs\n",
    "for epch in range(n_epochs):\n",
    "    \n",
    "    # Initialize the loss values to zero at the beginning of the epoch\n",
    "    train_loss = 0.\n",
    "    valid_loss = 0.\n",
    "\n",
    "    # Train for an epoch\n",
    "    for idx, (images, labels) in enumerate(training_generator, start = 1):\n",
    "        images, labels = images[0].to(DEVICE), labels.to(DEVICE)\n",
    "        feature_vectors = model(images)\n",
    "        loss = loss_func(feature_vectors, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = round_off(loss.item())\n",
    "        train_loss += batch_loss\n",
    "        \n",
    "        if (idx % PRINT_PROGRESS == 0) or (idx == 0) or (idx == n_train_batches - 1):\n",
    "            print(f\"Epoch: {(epch + 1):<4}| Batch Number: {idx:<4}| Current Batch Loss: {batch_loss:<7}| Average Train Loss: {round_off(train_loss / idx):<7}\")\n",
    "    \n",
    "    # Validate after the trained epoch\n",
    "    for images, labels in validation_generator:\n",
    "        images, labels = images[0].to(DEVICE), labels.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            feature_vectors = model(images)\n",
    "            loss = loss_func(feature_vectors, labels)\n",
    "            valid_loss += round_off(loss.item())\n",
    "    \n",
    "    # Average the train and valid losses across all batches and save it to our array\n",
    "    train_loss = round_off(train_loss / n_train_batches)\n",
    "    valid_loss = round_off(valid_loss / n_valid_batches)\n",
    "    \n",
    "    print(f\"####################### End of Epoch {epch + 1} #######################\")\n",
    "    print(f\"Epoch: {(epch + 1):<4}| Train Loss: {train_loss:<7}| Valid Loss: {valid_loss:<7}\")\n",
    "    print()\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    # Check the valid loss and reduce learning rate as per the need\n",
    "    scheduler.step(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the losses to a loss_history.csv file on the disk\n",
    "history = pd.DataFrame({\"train_loss\": train_losses, \"valid_loss\":valid_losses})\n",
    "history.to_csv(\"loss_history.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to our disk\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
